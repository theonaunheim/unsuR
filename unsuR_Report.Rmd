---
title: "Risk Assessment Report"
output: 
  flexdashboard::flex_dashboard:
    logo: imgs/emblem.png
    favicon: imgs/favicon.png
    theme: united
---

Author: Corey Neskey, CISSP | cneskey@protonmail.com | https://github.com/cneskey | https://linkedin.com/in/cneskey | https://github.com/cneskey/unsuR | @cneskey

```{r resources, include=FALSE}
library(flexdashboard)
library(plyr)
library(googledrive)
library(ggplot2)
library(scales)
library(reshape2)
library(viridis)
library(httpuv)
library(ggridges)
library(readxl)
library(plotly)
library(dplyr)
library(formattable)
library(sparkline)
library(plotly)
library(stringr)
library(ggbeeswarm)
library(kableExtra)
library(leaflet)

source("src/rpert.R")
source("src/monterlo.R")
set.seed(3141593)

# If having Pandoc errors you may need to create a file that explicitly tells R where to launch from. This will happen if you have a weird install or are using a virtual workstation etc.
# Sys.getenv('R_USER')
# .Renviron file in H:/ and included in it the single entry R_USER=H:/
```

```{r parameters, include=FALSE}
# Activate any custom functions/modules source
# Set ####
  # Set number of simulation variations (at least 10,000 recommended)
  n_perms = 10000

  # drive_auth(email = "cneskey@gmail.com")
  # drive_user()
  # # Identify the sheet of interest
  # gsheet_name <- drive_get("https://docs.google.com/spreadsheets/d/1DWB4rdAmUGggkUN0KtVdtyn6E1gZ9j68QifDD8cn2fY/edit#gid=1146850498")
  # # Download the sheet as 'download_temp', overwrite if present.
  # drive_download(gsheet_name, path = "downloaded_temp", overwrite = TRUE)


```
  
```{r preprocessing, include=FALSE}
# read the "api_stages" sheets into memory.
  gEstimates <- read_excel("downloaded_temp.xlsx", sheet = "api_stage", skip = 10)
  gEstimates2 <- read_excel("downloaded_temp.xlsx", sheet = "api_stage_2", skip = 9)
  gScope <- read_excel("downloaded_temp.xlsx", sheet = "Scope", skip = 3)
  gCommentary <- read_excel("downloaded_temp.xlsx", sheet = "Commentary")
  
# Commentary  
  gCommentaryH <- gCommentary$Subsection
  gCommentaryF <- t(gCommentary$Commentary)
  colnames(gCommentaryF) <- gCommentaryH
  gCommentaryF <- as.data.frame(gCommentaryF)
  
# Sim ----
# Define unique identifier variables
  n_scens = length(na.omit(gEstimates$`UID`))
  n_bens = length(na.omit(gEstimates2$`Benefit UID`))
  n_costs = length(na.omit(gEstimates2$`Known Costs UID`))
  
# Declare necessary variables
  sim_output_A_FRQ <- data.frame() 
  sim_output_B_FRQ <- data.frame()
  sim_output_C_FRQ <- data.frame()
  sim_output_A <- data.frame()
  sim_output_B <- data.frame()
  sim_output_C <- data.frame()
  sim_output_A_ICC <- data.frame()
  sim_output_B_ICC <- data.frame()
  sim_output_C_ICC <- data.frame()
  sim_output_A_RCC <- data.frame()
  sim_output_B_RCC <- data.frame()
  sim_output_C_RCC <- data.frame()
  sim_output_Bens <- data.frame()
  sim_output_Costs <- data.frame()

# Convert estimates from Frequency Formats to decimal / percentage ----
  # Plan A
  freq_form <- gEstimates$`Plan A Loss Event Frequency (LEF) Lower Bound`
  freq_form <- str_split(freq_form, " in ", n=2, simplify = TRUE)
  perc_form <- as.numeric(freq_form[,1])/as.numeric(freq_form[,2])
  gEstimates$`Plan A Loss Event Frequency (LEF) Lower Bound` <- perc_form
  
  freq_form <- gEstimates$`Plan A Loss Event Frequency (LEF) Most Likely`
  freq_form <- str_split(freq_form, " in ", n=2, simplify = TRUE)
  perc_form <- as.numeric(freq_form[,1])/as.numeric(freq_form[,2])
  gEstimates$`Plan A Loss Event Frequency (LEF) Most Likely` <- perc_form
  
  freq_form <- gEstimates$`Plan A Loss Event Frequency (LEF) Upper Bound`
  freq_form <- str_split(freq_form, " in ", n=2, simplify = TRUE)
  perc_form <- as.numeric(freq_form[,1])/as.numeric(freq_form[,2])
  gEstimates$`Plan A Loss Event Frequency (LEF) Upper Bound` <- perc_form
  
  # Plan B
  freq_form <- gEstimates$`Plan B Loss Event Frequency (LEF) Lower Bound`
  freq_form <- str_split(freq_form, " in ", n=2, simplify = TRUE)
  perc_form <- as.numeric(freq_form[,1])/as.numeric(freq_form[,2])
  gEstimates$`Plan B Loss Event Frequency (LEF) Lower Bound` <- perc_form
  
  freq_form <- gEstimates$`Plan B Loss Event Frequency (LEF) Most Likely`
  freq_form <- str_split(freq_form, " in ", n=2, simplify = TRUE)
  perc_form <- as.numeric(freq_form[,1])/as.numeric(freq_form[,2])
  gEstimates$`Plan B Loss Event Frequency (LEF) Most Likely` <- perc_form
  
  freq_form <- gEstimates$`Plan B Loss Event Frequency (LEF) Upper Bound`
  freq_form <- str_split(freq_form, " in ", n=2, simplify = TRUE)
  perc_form <- as.numeric(freq_form[,1])/as.numeric(freq_form[,2])
  gEstimates$`Plan B Loss Event Frequency (LEF) Upper Bound` <- perc_form
  
  # Plan C
  freq_form <- gEstimates$`Plan C Loss Event Frequency (LEF) Lower Bound`
  freq_form <- str_split(freq_form, " in ", n=2, simplify = TRUE)
  perc_form <- as.numeric(freq_form[,1])/as.numeric(freq_form[,2])
  gEstimates$`Plan C Loss Event Frequency (LEF) Lower Bound` <- perc_form
  
  freq_form <- gEstimates$`Plan C Loss Event Frequency (LEF) Most Likely`
  freq_form <- str_split(freq_form, " in ", n=2, simplify = TRUE)
  perc_form <- as.numeric(freq_form[,1])/as.numeric(freq_form[,2])
  gEstimates$`Plan C Loss Event Frequency (LEF) Most Likely` <- perc_form
  
  freq_form <- gEstimates$`Plan C Loss Event Frequency (LEF) Upper Bound`
  freq_form <- str_split(freq_form, " in ", n=2, simplify = TRUE)
  perc_form <- as.numeric(freq_form[,1])/as.numeric(freq_form[,2])
  gEstimates$`Plan C Loss Event Frequency (LEF) Upper Bound` <- perc_form
```

```{r simulation, include=FALSE}
  
# Simulate Plan A FREQUENCY (Monte Carlo)
sim_output_A_FRQ <- monterlo(
  n_scens = n_scens,
  n_perms = n_perms,
  prb = rep(1,n_scens),
  mn = gEstimates$`Plan A Loss Event Frequency (LEF) Lower Bound`,
  ml = gEstimates$`Plan A Loss Event Frequency (LEF) Most Likely`,
  mx = gEstimates$`Plan A Loss Event Frequency (LEF) Upper Bound`,
  out_var = sim_output_A_FRQ)

# Simulate Plan B FREQUENCY (Monte Carlo)
sim_output_B_FRQ <- monterlo(
  n_scens = n_scens,
  n_perms = n_perms,
  prb = rep(1,n_scens),
  mn = gEstimates$`Plan B Loss Event Frequency (LEF) Lower Bound`,
  ml = gEstimates$`Plan B Loss Event Frequency (LEF) Most Likely`,
  mx = gEstimates$`Plan B Loss Event Frequency (LEF) Upper Bound`,
  out_var = sim_output_B_FRQ)

# Simulate Plan C FREQUENCY (Monte Carlo)
sim_output_C_FRQ <- monterlo(
  n_scens = n_scens,
  n_perms = n_perms,
  prb = rep(1,n_scens),
  mn = gEstimates$`Plan C Loss Event Frequency (LEF) Lower Bound`,
  ml = gEstimates$`Plan C Loss Event Frequency (LEF) Most Likely`,
  mx = gEstimates$`Plan C Loss Event Frequency (LEF) Upper Bound`,
  out_var = sim_output_C_FRQ)

# Load Frequency means into memory
FRQ_mean <- colMeans(sim_output_A_FRQ)
FRQ_mean <- cbind(FRQ_mean,colMeans(sim_output_B_FRQ))
FRQ_mean <- cbind(FRQ_mean,colMeans(sim_output_C_FRQ))
FRQ_mean <- as.data.frame(FRQ_mean)
colnames(FRQ_mean) <- c("LEF-A","LEF-B","LEF-C")

# Simulate Plan A MAGNITUDE & given FREQUENCY simulated in previous step (Monte Carlo)
  sim_output_A <- monterlo(
    n_scens = n_scens,
    n_perms = n_perms,
    prb = FRQ_mean$`LEF-A`,
    mn =  gEstimates$`Plan A Loss Magnitude (LM) Lower Bound`,
    ml = gEstimates$`Plan A Loss Magnitude (LM) Most Likely`,
    mx = gEstimates$`Plan A Loss Magnitude (LM) Upper Bound`,
    out_var = sim_output_A)
  
# Simulate Plan B MAGNITUDE & given FREQUENCY simulated in previous step (Monte Carlo)
  sim_output_B <- monterlo(
    n_scens = n_scens,
    n_perms = n_perms,
    prb = FRQ_mean$`LEF-B`,
    mn = gEstimates$`Plan B Loss Magnitude (LM) Lower Bound`,
    ml = gEstimates$`Plan B Loss Magnitude (LM) Most Likely`,
    mx = gEstimates$`Plan B Loss Magnitude (LM) Upper Bound`,
    out_var = sim_output_B)
  
# Simulate Plan C MAGNITUDE & given FREQUENCY simulated in previous step (Monte Carlo)
  sim_output_C <- monterlo(
    n_scens = n_scens,
    n_perms = n_perms,
    prb = FRQ_mean$`LEF-C`,
    mn = gEstimates$`Plan C Loss Magnitude (LM) Lower Bound`,
    ml = gEstimates$`Plan C Loss Magnitude (LM) Most Likely`,
    mx = gEstimates$`Plan C Loss Magnitude (LM) Upper Bound`,
    out_var = sim_output_C)
  
  # Simulate Initial Control Costs
    # Simulate Plan A initial CONTROL Costs
    sim_output_A_ICC <- monterlo(
      n_scens = n_scens,
      n_perms = n_perms,
      prb = rep(1,n_scens),
      mn = gEstimates$`Plan A Initial Control Cost Lower Bound`,
      ml = gEstimates$`Plan A Initial Control Cost Most Likely`,
      mx = gEstimates$`Plan A Initial Control Cost Upper Bound`,
      out_var = sim_output_A_ICC)
    
    # Simulate Plan B  initial CONTROL Costs
    sim_output_B_ICC <- monterlo(
      n_scens = n_scens,
      n_perms = n_perms,
      prb = rep(1,n_scens),
      mn = gEstimates$`Plan B Initial Control Cost Lower Bound`,
      ml = gEstimates$`Plan B Initial Control Cost Most Likely`,
      mx = gEstimates$`Plan B Initial Control Cost Upper Bound`,
      out_var = sim_output_B_ICC)
    
    # Simulate Plan C initial CONTROL Costs
    sim_output_C_ICC <- monterlo(
      n_scens = n_scens,
      n_perms = n_perms,
      prb = rep(1,n_scens),
      mn = gEstimates$`Plan C Initial Control Cost Lower Bound`,
      ml = gEstimates$`Plan C Initial Control Cost Most Likely`,
      mx = gEstimates$`Plan C Initial Control Cost Upper Bound`,
      out_var = sim_output_C_ICC)
    
    # Simulate Recurring Control Costs
    # Simulate Plan A recurring CONTROL Costs
    sim_output_A_RCC <- monterlo(
      n_scens = n_scens,
      n_perms = n_perms,
      prb = rep(1,n_scens),
      mn = gEstimates$`Plan A Recurring Control Cost Lower Bound`,
      ml = gEstimates$`Plan A Recurring Control Cost Most Likely`,
      mx = gEstimates$`Plan A Recurring Control Cost Upper Bound`,
      out_var = sim_output_A_RCC)
    
    # Simulate Plan B recurring CONTROL Costs
    sim_output_B_RCC <- monterlo(
      n_scens = n_scens,
      n_perms = n_perms,
      prb = rep(1,n_scens),
      mn = gEstimates$`Plan B Recurring Control Cost Lower Bound`,
      ml = gEstimates$`Plan B Recurring Control Cost Most Likely`,
      mx = gEstimates$`Plan B Recurring Control Cost Upper Bound`,
      out_var = sim_output_B_RCC)
    
    # Simulate Plan C recurring CONTROL Costs
    sim_output_C_RCC <- monterlo(
      n_scens = n_scens,
      n_perms = n_perms,
      prb = rep(1,n_scens),
      mn = gEstimates$`Plan C Recurring Control Cost Lower Bound`,
      ml = gEstimates$`Plan C Recurring Control Cost Most Likely`,
      mx = gEstimates$`Plan C Recurring Control Cost Upper Bound`,
      out_var = sim_output_C_RCC)
  
    # Simulate Known Benefits
    sim_output_Bens <- monterlo(
      n_scens = n_bens,
      n_perms = n_perms,
      prb = as.numeric(na.omit(gEstimates2$`Benefits Probability`)),
      mn = as.integer(na.omit(gEstimates2$`Benefits Lower Bound`)),
      ml = as.integer(na.omit(gEstimates2$`Benefits Most Likely`)),
      mx = as.integer(na.omit(gEstimates2$`Benefits Upper Bound`)),
      out_var = sim_output_Bens)
    
    # Simulated Known Costs
    sim_output_Costs <- monterlo(
      n_scens = n_costs,
      n_perms = n_perms,
      prb = rep(1,n_scens),
      mn = gEstimates2$`Known Costs Lower Bound`,
      ml = gEstimates2$`Known Costs Most Likely`,
      mx = gEstimates2$`Known Costs Upper Bound`,
      out_var = sim_output_Costs)
```

```{r evaluations, include=FALSE}
# Add header rows to simulation outputs
  # headers for loss outputs
  colnames(sim_output_A) <- as.character(paste("Risk-",1:n_scens,sep = ""))
  colnames(sim_output_B) <- as.character(paste("Risk-",1:n_scens,sep = ""))
  colnames(sim_output_C) <- as.character(paste("Risk-",1:n_scens,sep = ""))
  # headers for initial control cost outputs
  colnames(sim_output_A_ICC) <- as.character(paste("Risk-",1:n_scens,sep = ""))
  colnames(sim_output_B_ICC) <- as.character(paste("Risk-",1:n_scens,sep = ""))
  colnames(sim_output_C_ICC) <- as.character(paste("Risk-",1:n_scens,sep = ""))
  # headers for recurring control cost outputs
  colnames(sim_output_A_RCC) <- as.character(paste("Risk-",1:n_scens,sep = ""))
  colnames(sim_output_B_RCC) <- as.character(paste("Risk-",1:n_scens,sep = ""))
  colnames(sim_output_C_RCC) <- as.character(paste("Risk-",1:n_scens,sep = ""))
  # headers for benefit outputs
  colnames(sim_output_Bens) <- as.character(paste("benefit-",1:n_bens,sep = ""))
  # headers for benefit outputs
  colnames(sim_output_Costs) <- as.character(paste("cost-",1:n_costs,sep = ""))

# Load the mean of all columns into memory
  # load Loss Magnitude (LM) means into memory
  risk_mean <- colMeans(sim_output_A)
  risk_mean <- cbind(risk_mean,colMeans(sim_output_B))
  risk_mean <- cbind(risk_mean,colMeans(sim_output_C))
  risk_mean <- as.data.frame(risk_mean)
  colnames(risk_mean) <- c("PLAN-A","PLAN-B","PLAN-C")
  
  # load Initial control cost (ICC) means into memory
  control_mean <- colMeans(sim_output_A_ICC)
  control_mean <- cbind(control_mean,colMeans(sim_output_B_ICC))
  control_mean <- cbind(control_mean,colMeans(sim_output_C_ICC))

  # load recurring control cost (RCC) means into memory
  control_mean <- cbind(control_mean,colMeans(sim_output_A_RCC))
  control_mean <- cbind(control_mean,colMeans(sim_output_B_RCC))
  control_mean <- cbind(control_mean,colMeans(sim_output_C_RCC))
  
  # convert control means object to dataframe and name columns
  control_mean <- as.data.frame(control_mean)
  colnames(control_mean) <- c("ICC-A","ICC-B","ICC-C","RCC-A","RCC-B","RCC-C")
  
  # load Benefit means into memory
  Benefit_mean <- colMeans(sim_output_Bens)
  Benefit_mean <- as.data.frame(Benefit_mean)
  # eval recurring
  Benefit_mean <- cbind(Benefit_mean,Benefit_mean*gEstimates2$`Benefits Recurring_Ben`)
  colnames(Benefit_mean) <- c("Benefit","Recurring_Benefit")
  
  # load Costs means into memory
  Cost_mean <- colMeans(sim_output_Costs)
  Cost_mean <- as.data.frame(Cost_mean)
  # eval recurring
  Cost_mean <- cbind(Cost_mean,Cost_mean*gEstimates2$`Known Costs Recurring Expense`)
  colnames(Cost_mean) <- c("Cost","Recurring_Cost")
  
# Eval ----
  evals <- data.frame()
  REL_AvB <- risk_mean$`PLAN-A` - risk_mean$`PLAN-B`
  REL_AvC <- risk_mean$`PLAN-A` - risk_mean$`PLAN-C`
  ROSI_AvB <- na_if(na_if(REL_AvB/control_mean$`ICC-B`-1,"Inf"),"-Inf")
  ROSI_AvC <- na_if(na_if(REL_AvC/control_mean$`ICC-C`-1,"Inf"),"-Inf")
  NET_AvB <- na_if(na_if(ROSI_AvB*control_mean$`ICC-B`,"Inf"),"-Inf")
  NET_AvC <- na_if(na_if(ROSI_AvC*control_mean$`ICC-C`,"Inf"),"-Inf")
  
  # Forcasts
    # Year 1
      # A: Benefits - Costs - Loss_magn.
        NET_A_Year1 <- sum(Benefit_mean$Benefit) - sum(Cost_mean$Cost) - risk_mean$`PLAN-A`
      # B: Benefits_init - Costs_init - Loss Magn. - Ctrl_cost_init + Reduction_In_exp_losses
        NET_B_Year1 <- sum(Benefit_mean$Benefit) - sum(Cost_mean$Cost) - risk_mean$`PLAN-B` - control_mean$`ICC-B` + REL_AvB
      # C: Benefits_init - Costs_init - Loss Magn. - Ctrl_cost_init + Reduction_In_exp_losses
        NET_C_Year1 <- sum(Benefit_mean$Benefit) - sum(Cost_mean$Cost) - risk_mean$`PLAN-C` - control_mean$`ICC-C` + REL_AvC
        
    # Year 2
      # A: (((Benefits+Benefits_recurring)-(Costs-Costs_Recurring))-(Loss_Magn*2))
        NET_A_Year2 <- (((sum(Benefit_mean$Benefit)+sum(Benefit_mean$Benefit_Recurring)) - (sum(Cost_mean$Cost-sum(Cost_mean$Cost_Recurring))) - risk_mean$`PLAN-A`*2))
        
      # B: (((Benefits+Benefits_recurring)-(Costs-Costs_Recurring))-(Loss_Magn*2-Initial_Ctrl_Cost+ReductionEL*2))-Ctrl_Costs_recurring
        NET_B_Year2 <- (((sum(Benefit_mean$Benefit)+sum(Benefit_mean$Benefit_Recurring)) - (sum(Cost_mean$Cost-sum(Cost_mean$Cost_Recurring))) - risk_mean$`PLAN-B`*2 - control_mean$`ICC-B` + REL_AvB*2)) - control_mean$`RCC-B`
        
      # C: (((Benefits+Benefits_recurring)-(Costs-Costs_Recurring))-(Loss_Magn*2-Initial_Ctrl_Cost+ReductionEL*2))-Ctrl_Costs_recurring
        NET_C_Year2 <- (((sum(Benefit_mean$Benefit)+sum(Benefit_mean$Benefit_Recurring)) - (sum(Cost_mean$Cost-sum(Cost_mean$Cost_Recurring))) - risk_mean$`PLAN-C`*2 - control_mean$`ICC-C` + REL_AvC*2)) - control_mean$`RCC-C`
        
    # Year 3
      # A: (Benefit+Benefit_recurring*2)-(Cost_initial+Cost_recurring*2)-Loss_magn*3-Ctrl_Cost_init)-Ctrl_Cost_recurring*2
        NET_A_Year3 <- (((sum(Benefit_mean$Benefit)+sum(Benefit_mean$Benefit_Recurring*2)) - (sum(Cost_mean$Cost-sum(Cost_mean$Cost_Recurring*2))) - risk_mean$`PLAN-A`*3 - control_mean$`ICC-A`)) - control_mean$`RCC-A`*2
        
      # B: (Benefit+Benefit_recurring*2)-(Cost_initial+Cost_recurring*2)-Loss_magn*3-Ctrl_Cost_init+ReductionEL*3)-Ctrl_Cost_recurring*2
        NET_B_Year3 <- (((sum(Benefit_mean$Benefit)+sum(Benefit_mean$Benefit_Recurring*2)) - (sum(Cost_mean$Cost-sum(Cost_mean$Cost_Recurring*2))) - risk_mean$`PLAN-A`*3 - control_mean$`ICC-B` + REL_AvB*3)) - control_mean$`RCC-B`*2
        
      # C: (Benefit+Benefit_recurring*2)-(Cost_initial+Cost_recurring*2)-Loss_magn*3-Ctrl_Cost_init+ReductionEL*3)-Ctrl_Cost_recurring*2
        NET_C_Year3 <- (((sum(Benefit_mean$Benefit)+sum(Benefit_mean$Benefit_Recurring*2)) - (sum(Cost_mean$Cost-sum(Cost_mean$Cost_Recurring*2))) - risk_mean$`PLAN-A`*3 - control_mean$`ICC-C` + REL_AvB*3)) - control_mean$`RCC-C`*2
      
  evals <- cbind(risk_mean,
                 control_mean,
                 REL_AvB,
                 REL_AvC,
                 ROSI_AvB,
                 ROSI_AvC,
                 NET_AvB,
                 NET_AvC,
                 NET_A_Year1,
                 NET_B_Year1,
                 NET_C_Year1,
                 NET_A_Year2,
                 NET_B_Year2,
                 NET_C_Year2,
                 NET_A_Year3,
                 NET_B_Year3,
                 NET_C_Year3
                 )

  # Year 1 Expected
    Expected_Benefits_y1 <- sum(Benefit_mean$Benefit)
    Expected_Implementation_Costs_y1 <- sum(Cost_mean$Cost)
    
    # Plan A
    Expected_Losses_A_y1 <- mean(risk_mean$`PLAN-A`)
    Expected_Mitigation_Costs_A_y1 <- mean(control_mean$`ICC-A`)
    Expected_Prevented_Loss_A_y1 <- 0
    Expected_Net_A_y1 <- mean(NET_A_Year1)
    
    # Plan B
    Expected_Losses_B_y1 <- mean(risk_mean$`PLAN-B`)
    Expected_Mitigation_Costs_B_y1 <- mean(control_mean$`ICC-B`)
    Expected_Prevented_Loss_B_y1 <- mean(REL_AvB)
    Expected_Net_B_y1 <- mean(NET_B_Year1)
    
    # Plan C
    Expected_Losses_C_y1 <- mean(risk_mean$`PLAN-C`)
    Expected_Mitigation_Costs_C_y1 <- mean(control_mean$`ICC-C`)
    Expected_Prevented_Loss_C_y1 <- mean(REL_AvC)
    Expected_Net_C_y1 <- mean(NET_C_Year1)
    
  # Year 2 Expected
    Expected_Benefits_y2 <- sum(Benefit_mean$Benefit)+sum(Benefit_mean$Recurring_Benefit)
    Expected_Implementation_Costs_y2 <- sum(Cost_mean$Cost)+sum(Cost_mean$Recurring_Cost)
    
    # Plan A
    Expected_Losses_A_y2 <- mean(risk_mean$`PLAN-A`)*2
    Expected_Mitigation_Costs_A_y2 <- mean(control_mean$`ICC-A`)+mean(control_mean$`RCC-A`)
    Expected_Prevented_Loss_A_y2 <- 0
    Expected_Net_A_y2 <- mean(NET_A_Year2)
    
    # Plan B
    Expected_Losses_B_y2 <- mean(risk_mean$`PLAN-B`)*2
    Expected_Mitigation_Costs_B_y2 <- mean(control_mean$`ICC-B`)+mean(control_mean$`RCC-B`)
    Expected_Prevented_Loss_B_y2 <- mean(REL_AvB)*2
    Expected_Net_B_y2 <- mean(NET_B_Year2)
    
    # Plan C
    Expected_Losses_C_y2 <- mean(risk_mean$`PLAN-C`)*2
    Expected_Mitigation_Costs_C_y2 <- mean(control_mean$`ICC-C`)+mean(control_mean$`RCC-C`)
    Expected_Prevented_Loss_C_y2 <- mean(REL_AvC)*2
    Expected_Net_C_y2 <- mean(NET_C_Year2)
    
  # Year 3 Expected
    Expected_Benefits_y3 <- sum(Benefit_mean$Benefit)+sum(Benefit_mean$Recurring_Benefit)*2
    Expected_Implementation_Costs_y3 <- sum(Cost_mean$Cost)+sum(Cost_mean$Recurring_Cost)*2
    
    # Plan A
    Expected_Losses_A_y3 <- mean(risk_mean$`PLAN-A`)*3
    Expected_Mitigation_Costs_A_y3 <- mean(control_mean$`ICC-A`)+mean(control_mean$`RCC-A`)*2
    Expected_Prevented_Loss_A_y3 <- 0
    Expected_Net_A_y3 <- mean(NET_A_Year3)
    
    # Plan B
    Expected_Losses_B_y3 <- mean(risk_mean$`PLAN-B`)*3
    Expected_Mitigation_Costs_B_y3 <- mean(control_mean$`ICC-B`)+mean(control_mean$`RCC-B`)*2
    Expected_Prevented_Loss_B_y3 <- mean(REL_AvB)*3
    Expected_Net_B_y3 <- mean(NET_B_Year3)
    
    # Plan C
    Expected_Losses_C_y3 <- mean(risk_mean$`PLAN-C`)*3
    Expected_Mitigation_Costs_C_y3 <- mean(control_mean$`ICC-C`)+mean(control_mean$`RCC-C`)*2
    Expected_Prevented_Loss_C_y3 <- mean(REL_AvC)*3
    Expected_Net_C_y3 <- mean(NET_C_Year3)
  
  # Build forcast tables
  # Plan A 3-year table
  Plan_A_Expected <- as.data.frame(rbind(
    Expected_Benefits_y1,
    Expected_Implementation_Costs_y1,
    Expected_Losses_A_y1,
    Expected_Mitigation_Costs_A_y1,
    Expected_Prevented_Loss_A_y1,
    Expected_Net_A_y1))
  Plan_A_Expected$Year_2 <- rbind(
    Expected_Benefits_y2,
    Expected_Implementation_Costs_y2,
    Expected_Losses_A_y2,
    Expected_Mitigation_Costs_A_y2,
    Expected_Prevented_Loss_A_y2,
    Expected_Net_A_y2)
  Plan_A_Expected$Year_3 <- rbind(
    Expected_Benefits_y3,
    Expected_Implementation_Costs_y3,
    Expected_Losses_A_y3,
    Expected_Mitigation_Costs_A_y3,
    Expected_Prevented_Loss_A_y3,
    Expected_Net_A_y3)
  
  # Plan B 3-year table
  Plan_B_Expected <- as.data.frame(rbind(
    Expected_Benefits_y1,
    Expected_Implementation_Costs_y1,
    Expected_Losses_B_y1,
    Expected_Mitigation_Costs_B_y1,
    Expected_Prevented_Loss_B_y1,
    Expected_Net_B_y1))
  Plan_B_Expected$Year_2 <- rbind(
    Expected_Benefits_y2,
    Expected_Implementation_Costs_y2,
    Expected_Losses_B_y2,
    Expected_Mitigation_Costs_B_y2,
    Expected_Prevented_Loss_B_y2,
    Expected_Net_B_y2)
  Plan_B_Expected$Year_3 <- rbind(
    Expected_Benefits_y3,
    Expected_Implementation_Costs_y3,
    Expected_Losses_B_y3,
    Expected_Mitigation_Costs_B_y3,
    Expected_Prevented_Loss_B_y3,
    Expected_Net_B_y3)
  
  # Plan c 3-year table
  Plan_C_Expected <- as.data.frame(rbind(
    Expected_Benefits_y1,
    Expected_Implementation_Costs_y1,
    Expected_Losses_C_y1,
    Expected_Mitigation_Costs_C_y1,
    Expected_Prevented_Loss_C_y1,
    Expected_Net_C_y1))
  Plan_C_Expected$Year_2 <- rbind(
    Expected_Benefits_y2,
    Expected_Implementation_Costs_y2,
    Expected_Losses_C_y2,
    Expected_Mitigation_Costs_C_y2,
    Expected_Prevented_Loss_C_y2,
    Expected_Net_C_y2)
  Plan_C_Expected$Year_3 <- rbind(
    Expected_Benefits_y3,
    Expected_Implementation_Costs_y3,
    Expected_Losses_C_y3,
    Expected_Mitigation_Costs_C_y3,
    Expected_Prevented_Loss_C_y3,
    Expected_Net_C_y3)
  
  # Name rows and columns
  row.names(Plan_A_Expected) <- c("Benefits","Costs","Loss","Mitigation Costs","Prevented Loss","Net")
  colnames(Plan_A_Expected) <- c("Year 1", "Year 2","Year 3")
  row.names(Plan_B_Expected) <- c("Benefits","Costs","Loss","Mitigation Costs","Prevented Loss","Net")
  colnames(Plan_B_Expected) <- c("Year 1", "Year 2","Year 3")
  row.names(Plan_C_Expected) <- c("Benefits","Costs","Loss","Mitigation Costs","Prevented Loss","Net")
  colnames(Plan_C_Expected) <- c("Year 1", "Year 2","Year 3")
  
```
  
```{r procesing_for_visuals, include=FALSE}
  #Data Prep for visualisations
  # Density Plot 1
  df <- t(risk_mean)
  df.m <- melt(df)
  colnames(df.m) <- as.character(c("plan","risk","loss"))
  mu <- ddply(df.m, "plan", summarise, grp.mean=mean(loss))
  df.m$plan <- factor(df.m$plan, levels = c("PLAN-C", "PLAN-B", "PLAN-A"))
  mu$plan <- factor(mu$plan, levels = c("PLAN-C", "PLAN-B", "PLAN-A"))
  
  # Density Plot 2 (costs)
  df2 <- t(Cost_mean)
  df.m2 <- melt(sim_output_Costs)
  colnames(df.m2) <- as.character(c("cost_name","cost_amt"))
  mu2 <- ddply(df.m2, "cost_name", summarise, grp.mean=mean(cost_amt))
  df.m2$cost_name <- factor(df.m2$cost_name)
  mu2$cost_name <- factor(mu2$cost_name)
  
  # Density Plot 3 (bens)
  df3 <- t(Benefit_mean)
  df.m3 <- melt(sim_output_Bens)
  colnames(df.m3) <- as.character(c("ben_name","ben_amt"))
  mu3 <- ddply(df.m3, "ben_name", summarise, grp.mean=mean(ben_amt))
  df.m3$ben_name <- factor(df.m3$ben_name)
  mu3$cost_name <- factor(mu3$ben_name)
  
# For per plan ECDF
  loss_ecdf <- ddply(df.m, c("plan"), mutate, ecdf = ecdf(loss)(unique(loss))*length(loss))
  # To invert the per plan ECDF
  loss_ecdf_2 <- ddply(loss_ecdf, "plan", mutate, ecdf = scale(ecdf,center=min(ecdf),scale=diff(range(ecdf))))
  
  # Net amounts and graphs
  Nets <- rbind(Plan_A_Expected[6,], Plan_B_Expected[6,], Plan_C_Expected[6,])
  row.names(Nets) <- c("PLAN-C", "PLAN-B", "PLAN-A")
  Netsm <- melt(t(Nets),id = 0)
  Netsm$value <- as.numeric(Netsm$value)
  Netsm$value <- dollar(as.numeric(Netsm$value))
  colnames(Netsm) <- c("Year", "Plan", "Net")
  
  colnames(gScope) <- c("Included","Excluded","Included","Excluded","Included","Excluded","Included","Excluded","Included","Excluded")
```

Assessment Methodology
=====================================

Col1 
-----------------------------------------

**Methodology Criteria**

A risk analysis should meet local, city, state, federal, and international compliance criteria and yield a corresponding risk assessment report. The criteria and objective of this analysis is as follows:  

1. To create a list of threats that the entity may become exposed to as a result of the changes presented in discussion with stakeholders.
2. To communicate the estimated probability and impact of such threats.
3. To create a list of controls/mitigation strategies that may reduce the probability, impact or uncertainty of the listed threats.
4. To communicate the measure of how much the probability, impact or uncertainty of the listed threats is modified by the controls/mitigation strategies considered.
5. To communicate the benefit of controls under consideration and costs associated with them.



**Methodology Standardization & Interoperability**

The taxonomy chosen is based on Open Group's Factor Analysis of Information Risk (FAIR) standard, an open and independent information risk analysis methodology. This ensures transparency, continuity, and interoperability with other major standards.


The Open Group is an industry consortium that facilitates business objectives by developing open, vendor-neutral technology standards and certifications.The Open Group published two Open FAIR standards that form the risk taxonomy followed:


  - [Open Risk Taxonomy Technical Standard (O-RT)](http://www.opengroup.org/library/C13K). This standard defines a standard taxonomy of terms, definitions, and relationships used in risk analysis.

 - [Open Risk Analysis Technical Standard (O-RA)](http://www.opengroup.org/library/C13G). This standard describes process aspects associated with performing effective risk analysis.
 
 
The [FAIR Institute](https://www.fairinstitute.org) maintains publicly available documentation, resources, community events and other modes of promotion, training, and collaboration.



**Deviations from Standard**
 
The methodology used for this assessment deviates from published standards where those standards deviate from scientifically rigorous literature that meets the following criteria:

 - Academic Journal (not trade journals),
 - Peer-reviewed,
 - Without conflicts of interest by for-profit entities, and
 - Without conflicts of interest by political entities.

An annotated review of the scientific literature supporting each component of this methodology may be found [here](https://sites.google.com/view/coreyneskey/improving-cybersecurity-decision-making-by-reducing-expert-bias).


**Methodology**

Scope definition, estimate parameters and commentary are collected using a format comfortable to most users, a spreadsheet. A [companion spreadsheet](https://docs.google.com/spreadsheets/d/1DWB4rdAmUGggkUN0KtVdtyn6E1gZ9j68QifDD8cn2fY/edit?usp=sharing) is provided with this tool which is interoperable with major spreadsheet rendering software such as Microsoft Office Excel and Google Sheets. The only variable that needs to be entered into this tool is the address or filepath to the [companion spreadsheet](https://docs.google.com/spreadsheets/d/1DWB4rdAmUGggkUN0KtVdtyn6E1gZ9j68QifDD8cn2fY/edit?usp=sharing) containing the scope components, estimate parameters, and desired commentary.

![](imgs/companion_spreadsheet_Google.png)

*[companion spreadsheet](https://docs.google.com/spreadsheets/d/1DWB4rdAmUGggkUN0KtVdtyn6E1gZ9j68QifDD8cn2fY/edit?usp=sharing) opened in Google Sheets.*



![](imgs/companion_spreadsheet_Microsoft.png)

*[companion spreadsheet](https://docs.google.com/spreadsheets/d/1DWB4rdAmUGggkUN0KtVdtyn6E1gZ9j68QifDD8cn2fY/edit?usp=sharing) opened in Microsoft Excel.*



Data is collected in the form of interviews, documentation review, and/or receptor-based discovery scanning in order to define the scope of the assessment. Abstractions of the components within scope are categorized into areas: Assets, Containers / Points of Attack, Agent / Threat Communities, Threat Types, and Threat Effects. 

![](imgs/scope_tab.png)

*NOTICE: Each column is an independent list. i.e. the contents of rows do not relate to each other.*


**Scenario Building**

Loss scenarios are generated by exhausting all combinations of the components identified as in scope. Implausible scenarios are removed e.g. non-malicious malware. Scenario components are strung together to form the respective scenario.

![](imgs/scenarios_table.png)



**Parameter Definition**

Probability and impact parameters are defined from the integration of data and [calibrated](https://sites.google.com/view/coreyneskey/improving-cybersecurity-decision-making-by-reducing-expert-bias#h.p_UeVP7PfPc-Qp) subject matter experts for each of the loss scenarios. Predefined distribution parameters and/or hyper-parameters of a loss event are used where they are available and credible.



![](imgs/params_table.png)


To take advantage of a person’s natural Bayesian tendencies, [calibration](https://sites.google.com/view/coreyneskey/improving-cybersecurity-decision-making-by-reducing-expert-bias#h.p_UeVP7PfPc-Qp) questions and responses take the form of [frequency formats](https://sites.google.com/view/coreyneskey/improving-cybersecurity-decision-making-by-reducing-expert-bias#h.p_WA7BG-Cgc50Q) instead of percentages or fractions.

[Frequency formats](https://sites.google.com/view/coreyneskey/improving-cybersecurity-decision-making-by-reducing-expert-bias#h.p_WA7BG-Cgc50Q) communicate information to experts in a form that more closely resembles the natural sampling observed in animal foraging and neural networks. What is 1% in standard format would be "10 in 100" in frequency format.



**Control Planning**

This risk assessment tool facilitates the comparison of different combinations of controls that may reduce the probability, impact, or uncertainty of loss events. The tool calls the first theortical combination of loss events and controls "Plan-A". Plan-A represents the absense of any controls in order to establish a baseline or "inherent risk". Plan-B is the second combination of controls. This is where analysts may list controls that are in place and additional controls that they are considering implementing. Plan C is where the analyst would enter an alternative set or combination of controls which require comparison.



![](imgs/control_planning.png)

After controls have been entered as column headers under "Controls" the check boxes are used to indicate which loss scenarios that control effects.

![](imgs/control_droids.png)

*e.g. The "Malware scans nightly" control is an applicable control to the Threat Community entries that contain "malicious software".*


**Simulation**

[Monte Carlo Simulation](https://sites.google.com/view/coreyneskey/improving-cybersecurity-decision-making-by-reducing-expert-bias#h.p_MF82ugY1c50h) is used to generate a dataset using the parameters provided. The simulations consist of at least 10,000 variations of each loss scenario.



![](imgs/monte_carlo_table.png)



**Analysis**

The resulting approximating dataset is then analyzed using appropriate statistical methodologies.

![](imgs/analysis_plots_snapshot.png)



**Reporting / Communication**

Background and scope may be communicated alone or alongside visuals by entering the desired text into the respective sections in the Commentary tab of the spreadsheet.

![](imgs/commentary_tab.png)

After analysis has concluded, conclusions and recommendations may also be communicated alone or alongside visuals by entering the desired text into the respective sections of the Commentary tab of the companion spreadsheet.

![](imgs/commentary_tab2.png)


Col2
-------------------------------------------------------------------------------------


Executive Summary {.storyboard}
=====================================

### **Background** {data-commentary-width=500}

![](imgs/graph(1).png)

***

```{r background} 
as.character(gCommentaryF$Background)
```

### **Scope** {data-commentary-width=500}

```{r scope_table2}
options(knitr.kable.NA = '')
kable(gScope, format = "html", caption = "Scope Table") %>%
  kable_styling(bootstrap_options = "striped",
                full_width = F) %>%
  add_header_above(c(
                     "Assets at risk"	= 2,
                     "Containers/Points of attack"= 2,
                     "Threat communities"= 2,
                     "Threat Types"= 2,
                     "Effects"= 2
                     ))
```


### **Recommendation**   {data-commentary-width=500}

```{r recommendation}
as.character(gCommentaryF$Recommendation)
```


### **Next Steps** {data-commentary-width=500}

```{r next_steps}
as.character(gCommentaryF$`Next Steps`)
```


Analysis {.storyboard}
=====================================

### **Projection**  The net value after factoring in costs, benefits, losses, and mitigation costs over 1 year, 2 year, and 3 years. {data-commentary-width=800}

**Plan A Expected**

```{r plan_a_expected}
a_exp <- formattable(mutate_all(Plan_A_Expected,dollar))
row.names(a_exp) <- row.names(Plan_A_Expected)
a_exp
```


**Plan B Expected**

```{r plan_b_expected}
b_exp <- formattable(mutate_all(Plan_B_Expected,dollar))
row.names(b_exp) <- row.names(Plan_B_Expected)
b_exp
```


**Plan C Expected**
 
```{r plan_c_expected}
c_exp <- formattable(mutate_all(Plan_C_Expected,dollar))
row.names(c_exp) <- row.names(Plan_C_Expected)
c_exp
```

***

```{r line_plot}
ggplotly(ggplot() +
  geom_line(data=Netsm, aes(x=Year,y=Net,group=Plan,colour=Plan))+
  geom_point(data=Netsm, aes(x=Year,y=Net,group=Plan,colour=Plan))+
  geom_hline(aes(yintercept = 0))+
  labs(
      title="Expected Net Value",
      subtitle = "Benefits minus Costs, Losses, Mitigation Costs, and Prevented Losses)",
      y = "Net Value")+
  scale_fill_viridis(discrete=TRUE)+
  scale_color_viridis(discrete=TRUE)
)

```

***

```{r projection}
as.character(gCommentaryF$Projection)
```

### **Benefits**  Parameters provided by experts to approximate benefits of this project {data-commentary-width=700}

```{r benefits_table}
options(knitr.kable.NA = '')

derp <- gEstimates2[,1:8]
derp[,4:6] <- formattable(mutate_all(derp[,4:6],currency, digits=0,))
derp[,3] <- formattable(mutate_all(derp[,3],percent, digits=0))
kable(derp, format = "html", caption = "Benefits Table") %>%
  kable_styling(bootstrap_options = "striped",
                full_width = F)
```

### **Costs** Parameters provided by experts to approximate the costs of this project. {data-commentary-width=700}

```{r costs_table}
options(knitr.kable.NA = '')
derp <- gEstimates2[,9:15]
derp[,3:5] <- formattable(mutate_all(derp[,3:5],currency, digits=0,))
kable(derp, format = "html", caption = "Costs Table") %>%
  kable_styling(bootstrap_options = "striped",
                full_width = F)
```

### Scenarios {data-commentary-width=500}
```{r Scenarios_table}
options(knitr.kable.NA = '')
derp <- gEstimates[,1:7]
kable(derp, format = "html", caption = "Scenarios Table") %>%
  kable_styling(bootstrap_options = "striped",
                full_width = F)
```

***
```{r next_steps_com}
as.character(gCommentaryF$`Next Steps`)
```

### ECDF {data-commentary-width=500}

```{r ECDF_plot}
  ####
  # # Inverse Empirical Cumulative Distribution Function (ECDF)
derp <- loss_ecdf_2
derp[3] <- formattable(mutate_all(derp[3], currency, digits=0))
derp[4] <- formattable(mutate_all(derp[4], percent, digits=0))
derp$`probability of exceeding` <- 1-derp$ecdf


  ggplotly(ggplot(derp,  mapping = aes(x = loss, y = `probability of exceeding`, colour = plan)) +
    geom_line()+
    labs(
      title="Simulated Loss Output",
      x = "Loss amount",
      y = "Probability of exceeding amount")+
    scale_x_continuous(labels = dollar_format())+
    scale_y_continuous(labels = percent_format())+
    scale_fill_viridis(discrete=TRUE)+
    scale_color_viridis(discrete=TRUE))

```

***
ECDF...

### Density {data-commentary-width=500}

```{r density_plot}
  # # Density Plot
  ggplotly(ggplot(derp, aes(x=loss, color=plan, fill=plan)) +
    geom_density(alpha=0.6) +
    geom_vline(data=mu, aes(xintercept=grp.mean, color=plan),
               linetype="dashed") +
    theme_classic() +
    theme(legend.position="top") +
    labs(
      title="Simulated Loss Output",
      subtitle = "Over a 12 month period",
      x = "Loss",
      y = "Density") +
    scale_x_continuous(labels = dollar_format())+
    scale_fill_viridis(discrete=TRUE)+
    scale_color_viridis(discrete=TRUE))

```

***
Density...

### Violin {data-commentary-width=500}

```{r violin_plot}
# # Vertical Violin Plot
  ggplotly(ggplot(data = df.m, aes(x=plan, y=loss)) +
    geom_violin(aes(fill = factor(plan))) +
    labs(
      title="Simulated Loss Output",
      subtitle = "Over a 12 month period",
      x = "Control Scenarios",
      y = "Loss") +
    scale_y_continuous(labels = dollar_format())+
    scale_fill_viridis(discrete=TRUE)+
    scale_color_viridis(discrete=TRUE))
```

***
Violin...

### Swarm {data-commentary-width=500}

```{r swarm_plot}
# # Vertical beeswarm Plot
  ggplotly(ggplot(data = df.m, aes(x=plan, y=loss,color = factor(plan))) +
             geom_beeswarm()+
    labs(
      title="Simulated Loss Output",
      subtitle = "Over a 12 month period",
      x = "Control Scenarios",
      y = "Loss") +
    scale_y_continuous(labels = dollar_format())+
    scale_fill_viridis(discrete=TRUE)+
    scale_color_viridis(discrete=TRUE))
```

***
Swarm...

### Box {data-commentary-width=500}

```{r box_plot}
# # Horizontal Box Plot
  ggplotly(ggplot(data = df.m, aes(x=plan, y=loss)) +
    geom_boxplot(outlier.colour = "black", outlier.shape = 1, color=c("#4b0082","lightseagreen","gold")) +
    labs(
      title="Simulated Loss Output",
      subtitle = "Over a 12 month period",
      x = "Control Scenarios",
      y = "Loss") +
    scale_y_continuous(labels = dollar_format()) +
    expand_limits(y = c(0,df.m$loss+(df.m$loss*.05))) +
    coord_flip())
```

***
Box...

### Ridge {data-commentary-width=500}

```{r ridge_plot}
# # Horizontal Ridge Density Plot
  ggplot(df.m, aes(x=loss, y=plan, fill=plan)) +
    geom_density_ridges(scale = .9, alpha = .8)+
    geom_vline(data=mu, aes(xintercept=grp.mean, color=plan),
               linetype="solid", show.legend = F) +
    labs(
      caption = "Vertical lines are loss means",
      title="Simulated Loss Output",
      subtitle = "Over a 12 month period",
      x = "Loss",
      y = "Density") +
    scale_x_continuous(labels = dollar_format())+
    scale_fill_viridis(discrete=TRUE) +
    scale_color_viridis(discrete=TRUE) +
    theme_ridges( grid = FALSE)
```

***

Ridge


Print Friendly
=====================================

THIS SECTION IS UNDER CONSTRUCTION

Assessment Methodology

**Methodology Criteria**

A risk analysis should meet local, city, state, federal, and international compliance criteria and yield a corresponding risk assessment report. The criteria and objective of this analysis is as follows:  

1. To create a list of threats that the entity may become exposed to as a result of the changes presented in discussion with stakeholders.
2. To communicate the estimated probability and impact of such threats.
3. To create a list of controls/mitigation strategies that may reduce the probability, impact or uncertainty of the listed threats.
4. To communicate the measure of how much the probability, impact or uncertainty of the listed threats is modified by the controls/mitigation strategies considered.
5. To communicate the benefit of controls under consideration and costs associated with them.



**Methodology Standardization & Interoperability**

The taxonomy chosen is based on Open Group's Factor Analysis of Information Risk (FAIR) standard, an open and independent information risk analysis methodology. This ensures transparency, continuity, and interoperability with other major standards.


The Open Group is an industry consortium that facilitates business objectives by developing open, vendor-neutral technology standards and certifications.The Open Group published two Open FAIR standards that form the risk taxonomy followed:


  - [Open Risk Taxonomy Technical Standard (O-RT)](http://www.opengroup.org/library/C13K). This standard defines a standard taxonomy of terms, definitions, and relationships used in risk analysis.

 - [Open Risk Analysis Technical Standard (O-RA)](http://www.opengroup.org/library/C13G). This standard describes process aspects associated with performing effective risk analysis.
 
 
The [FAIR Institute](https://www.fairinstitute.org) maintains publicly available documentation, resources, community events and other modes of promotion, training, and collaboration.



**Deviations from Standard**
 
The methodology used for this assessment deviates from published standards where those standards deviate from scientifically rigorous literature that meets the following criteria:

 - Academic Journal (not trade journals),
 - Peer-reviewed,
 - Without conflicts of interest by for-profit entities, and
 - Without conflicts of interest by political entities.

An annotated review of the scientific literature supporting each component of this methodology may be found [here](https://sites.google.com/view/coreyneskey/improving-cybersecurity-decision-making-by-reducing-expert-bias).


**Methodology**

Scope definition, estimate parameters and commentary are collected using a format comfortable to most users, a spreadsheet. A [companion spreadsheet](https://docs.google.com/spreadsheets/d/1DWB4rdAmUGggkUN0KtVdtyn6E1gZ9j68QifDD8cn2fY/edit?usp=sharing) is provided with this tool which is interoperable with major spreadsheet rendering software such as Microsoft Office Excel and Google Sheets. The only variable that needs to be entered into this tool is the address or filepath to the [companion spreadsheet](https://docs.google.com/spreadsheets/d/1DWB4rdAmUGggkUN0KtVdtyn6E1gZ9j68QifDD8cn2fY/edit?usp=sharing) containing the scope components, estimate parameters, and desired commentary.

![](imgs/companion_spreadsheet_Google.png)

*[companion spreadsheet](https://docs.google.com/spreadsheets/d/1DWB4rdAmUGggkUN0KtVdtyn6E1gZ9j68QifDD8cn2fY/edit?usp=sharing) opened in Google Sheets.*



![](imgs/companion_spreadsheet_Microsoft.png)

*[companion spreadsheet](https://docs.google.com/spreadsheets/d/1DWB4rdAmUGggkUN0KtVdtyn6E1gZ9j68QifDD8cn2fY/edit?usp=sharing) opened in Microsoft Excel.*



Data is collected in the form of interviews, documentation review, and/or receptor-based discovery scanning in order to define the scope of the assessment. Abstractions of the components within scope are categorized into areas: Assets, Containers / Points of Attack, Agent / Threat Communities, Threat Types, and Threat Effects. 

![](imgs/scope_tab.png)

*NOTICE: Each column is an independent list. i.e. the contents of rows do not relate to each other.*


**Scenario Building**

Loss scenarios are generated by exhausting all combinations of the components identified as in scope. Implausible scenarios are removed e.g. non-malicious malware. Scenario components are strung together to form the respective scenario.

![](imgs/scenarios_table.png)



**Parameter Definition**

Probability and impact parameters are defined from the integration of data and [calibrated](https://sites.google.com/view/coreyneskey/improving-cybersecurity-decision-making-by-reducing-expert-bias#h.p_UeVP7PfPc-Qp) subject matter experts for each of the loss scenarios. Predefined distribution parameters and/or hyper-parameters of a loss event are used where they are available and credible.

![](imgs/params_table.png)


To take advantage of a person’s natural Bayesian tendencies, [calibration](https://sites.google.com/view/coreyneskey/improving-cybersecurity-decision-making-by-reducing-expert-bias#h.p_UeVP7PfPc-Qp) questions and responses take the form of [frequency formats](https://sites.google.com/view/coreyneskey/improving-cybersecurity-decision-making-by-reducing-expert-bias#h.p_WA7BG-Cgc50Q) instead of percentages or fractions.

[Frequency formats](https://sites.google.com/view/coreyneskey/improving-cybersecurity-decision-making-by-reducing-expert-bias#h.p_WA7BG-Cgc50Q) communicate information to experts in a form that more closely resembles the natural sampling observed in animal foraging and neural networks. What is 1% in standard format would be "10 in 100" in frequency format.



**Control Planning**

This risk assessment tool facilitates the comparison of different combinations of controls that may reduce the probability, impact, or uncertainty of loss events. The tool calls the first theortical combination of loss events and controls "Plan-A". Plan-A represents the absense of any controls in order to establish a baseline or "inherent risk". Plan-B is the second combination of controls. This is where analysts may list controls that are in place and additional controls that they are considering implementing. Plan C is where the analyst would enter an alternative set or combination of controls which require comparison.



![](imgs/control_planning.png)

After controls have been entered as column headers under "Controls" the check boxes are used to indicate which loss scenarios that control effects.

![](imgs/control_droids.png)

*e.g. The "Malware scans nightly" control is an applicable control to the Threat Community entries that contain "malicious software".*


**Simulation**

[Monte Carlo Simulation](https://sites.google.com/view/coreyneskey/improving-cybersecurity-decision-making-by-reducing-expert-bias#h.p_MF82ugY1c50h) is used to generate a dataset using the parameters provided. The simulations consist of at least 10,000 variations of each loss scenario.



![](imgs/monte_carlo_table.png)



**Analysis**

The resulting approximating dataset is then analyzed using appropriate statistical methodologies.

![](imgs/analysis_plots_snapshot.png)



**Reporting / Communication**

Background and scope may be communicated alone or alongside visuals by entering the desired text into the respective sections in the Commentary tab of the spreadsheet.

![](imgs/commentary_tab.png)

After analysis has concluded, conclusions and recommendations may also be communicated alone or alongside visuals by entering the desired text into the respective sections of the Commentary tab of the companion spreadsheet.

![](imgs/commentary_tab2.png)



Executive Summary

**Background**

![](imgs/graph(1).png)


**Scope**

```{r ref.label="scope_table2"}
```


**Recommendation**  

```{r ref.label="recommendation"}
```


**Next Steps**

```{r ref.label="next_steps"}
```




**Analysis**

**Projection**  The net value after factoring in costs, benefits, losses, and mitigation costs over 1 year, 2 year, and 3 years.

**Plan A Expected**

```{r ref.label="plan_a_expected"}
```


**Plan B Expected**

```{r ref.label="plan_b_expected"}
```


**Plan C Expected**
 
```{r ref.label="plan_c_expected"}
```


```{r ref.label="line_plot"}
```


```{r ref.label="projection"}
```

**Benefits**  Parameters provided by experts to approximate benefits of this project

```{r ref.label="benefits_table"}
```

**Costs** Parameters provided by experts to approximate the costs of this project.

```{r ref.label="costs_table"}
```


```{r ref.label="next_steps_com"}
```


```{r ref.label="ECDF_plot"}
```


```{r ref.label="density_plot"}
```


```{r ref.label="violin_plot"}
```


```{r ref.label="swarm_plot"}
```


```{r ref.label="box_plot"}
```


```{r ref.label="ridge_plot"}
```

*Appendix A: Scenarios*
```{r ref.label="Scenarios_table"}
```